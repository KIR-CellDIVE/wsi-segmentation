{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Whole slide segmentation of Cell DIVE multiplex images using `DeepCell`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red\"> !!NOTE!! THIS NOTEBOOK ACTS AS A TEMPLATE AND WILL NOT SAFE CHANGES UNLESS YOU SAVE A COPY TO YOUR OWN FOLDER. USE `FILE->DOWNLOAD` IN THE TASKBAR AT THE VERY TOP OF THIS PAGE TO SAVE A COPY OF THE NOTEBOOK. THEN, USING THE SIDEBAR TO THE RIGHT OPEN THE NEW COPY OF THE NOTEBOOK BEFORE YOU GET STARTED (RUNNING ON WSL C: AND D: DRIVES SHOULD BE LOCATED UNDER `drives/c` AND `drives/d`, RESPECTIVELY) !!NOTE!!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run through this notebook **step-by-step** and adjust the code if necessary at any point in the process. The explaination and comments througout the notebook will help and guide through the proces. There will also be pointers as to what modification to a particular you might want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this notebook is adapted from and consequently loosely follows the structure and logic of the segmentation notebook of the `ark-analysis` toolset. Thus, it makes use some of the `ark-analysis` tooling as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We load all the necessary libraries used for the segmentation, visualisation and quantification in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as io\n",
    "import xarray as xr\n",
    "from alpineer import io_utils, load_utils, image_utils\n",
    "\n",
    "from ark.segmentation import marker_quantification, segmentation_utils\n",
    "from ark.utils import (deepcell_service_utils, plot_utils)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from deepcell.applications import Mesmer\n",
    "from deepcell.utils.plot_utils import make_outline_overlay\n",
    "from deepcell.utils.plot_utils import create_rgb_image\n",
    "\n",
    "from wsi_segmentation.segmentation_utils import predict_tiled\n",
    "from wsi_segmentation.io_utils import save_model_output_wrapper\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define data location and setup directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to set the *data/base directory* `base_dir` that contains a folder called `ome_tiff` with 1 *ome.tiff* per slide. The content of the *base* directory would look something like example below. We will also save the results and all the pipeline outputs in this folder as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "└── ome_tiff\n",
    "    ├── slide_asdfghjk.ome.tiff\n",
    "    └── slide_qwertyui.ome.tiff\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define the *data/base directory* we have two options:\n",
    "* set `set_base_dir_method=\"FilePicker\"` and use the widget to find and identify the data/base directory\n",
    "* set `set_base_dir_method=\"Manual\"` and manual define the data/base directory below via `data_dir=`\n",
    "* set `set_base_dir_method=\"Relative\"` which assume that this notebook is saved in a `notebook` folder alongside the `ome_tiff` in the same parent directory\n",
    "\n",
    "The last option `` ensure greatest reproducibility and transferability as the notebook is saved alongside the data and all path are defined relative to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: In `WSL` the `C:` drive, `D:` drive, etc are mounted and located at `/mnt/c`, `/mnt/d`, etc, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_base_dir_method = \"Relative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if set_base_dir_method == \"FilePicker\":\n",
    "    fc = FileChooser(\"\")\n",
    "    fc.show_only_dirs = True\n",
    "    fc.title = '<b>Choose data folder that contains `ome_tiff` folder with 1 ome_tiff per slide</b>'\n",
    "    display(fc)\n",
    "elif set_base_dir_method == \"Relative\":\n",
    "    notebook_dir = %pwd\n",
    "    data_dir = os.path.dirname(notebook_dir)\n",
    "elif set_base_dir_method == \"Manual\":\n",
    "    data_dir = \"/path/to/data/directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "base_dir"
    ]
   },
   "outputs": [],
   "source": [
    "# set the base directory\n",
    "base_dir = fc.selected_path if set_base_dir_method == \"FilePicker\" else data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup intermediate and results paths\n",
    "\n",
    "Next, we create all the required output and results directories such that we have the following folder structure in the `base directory`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── ome_tiff\n",
    "├── image_data\n",
    "└── segmentation\n",
    "    ├── cell_table\n",
    "    ├── cell_table_no_qc\n",
    "    ├── deepcell_input\n",
    "    ├── deepcell_output\n",
    "    ├── deepcell_output_no_qc\n",
    "    ├── deepcell_visualization\n",
    "    └── deepcell_visualization_no_qc\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set all the directory names to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "file_path"
    ]
   },
   "outputs": [],
   "source": [
    "ome_tiff_dir = os.path.join(base_dir, \"ome_tiff\")\n",
    "tiff_dir = os.path.join(base_dir, \"image_data\")\n",
    "cell_table_dir = os.path.join(base_dir, \"segmentation/cell_table_no_qc\")\n",
    "cell_table_dir_qc = os.path.join(base_dir, \"segmentation/cell_table\")\n",
    "deepcell_input_dir = os.path.join(base_dir, \"segmentation/deepcell_input\")\n",
    "deepcell_output_dir = os.path.join(base_dir, \"segmentation/deepcell_output_no_qc\")\n",
    "deepcell_output_dir_qc = os.path.join(base_dir, \"segmentation/deepcell_output\")\n",
    "deepcell_visualization_dir = os.path.join(base_dir, \"segmentation/deepcell_visualization_no_qc\")\n",
    "deepcell_visualization_dir_qc = os.path.join(base_dir, \"segmentation/deepcell_visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check if the directories exist and if not create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_dirs"
    ]
   },
   "outputs": [],
   "source": [
    "for directory in [tiff_dir, cell_table_dir, cell_table_dir_qc, deepcell_input_dir, deepcell_output_dir, deepcell_output_dir_qc, deepcell_visualization_dir, deepcell_visualization_dir_qc]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate that all the directories have been created correctly, so we can proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validate_path"
    ]
   },
   "outputs": [],
   "source": [
    "# validate paths\n",
    "io_utils.validate_paths([base_dir,\n",
    "                         tiff_dir,\n",
    "                         deepcell_input_dir,\n",
    "                         deepcell_output_dir,\n",
    "                         deepcell_output_dir_qc,\n",
    "                         cell_table_dir,\n",
    "                         cell_table_dir_qc,\n",
    "                         deepcell_visualization_dir,\n",
    "                         deepcell_visualization_dir_qc\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split OME TIFF files into single-channel TIFFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have obtained multichannel `ome.tiff` files from the Cell DIVE platform, we have to split them into single channel `tiff` files. To achieve this, set `split_ome = True` and split the `ome.tiff` by running the below steps. If you have run this step previously and already split up the `ome.tiff` files you can set `split_ome = False` to skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alternatively, you can also manually create one folder per image/slide containing single channel `tiff`s inside the `image_data` folder in the `base/data folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_ome = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if split_ome:\n",
    "    # Gather all the OME-TIFFs\n",
    "    ome_dir = pathlib.Path(ome_tiff_dir)\n",
    "    ome_tiffs = list(ome_dir.glob(\"*.ome.[tif tiff]*\"))\n",
    "\n",
    "    os.listdir(ome_tiff_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert all the OME-TIFFs to Single Channel TIFFs\n",
    "if split_ome:\n",
    "    for ome_tiff in tqdm(ome_tiffs):\n",
    "        load_utils.ome_to_fov(ome_tiff, data_dir=tiff_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these steps we should have one directory per slide in the `image_data` directory containing single `.tiff` files per channel. Looking similiar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── ome_tiff\n",
    "│   ├── slide_asdfghjk.ome.tiff\n",
    "│   └── slide_qwertyui.ome.tiff\n",
    "└── image_data\n",
    "    ├── slide_asdfghjk\n",
    "    │   ├── ChannelNameA.tiff\n",
    "    │   ├── ChannelNameB.tiff\n",
    "    │   ├── ChannelNameC.tiff\n",
    "    │   ├── ChannelNameD.tiff\n",
    "    │   ├── ChannelNameE.tiff\n",
    "    │   ├── ChannelNameF.tiff\n",
    "    │   ├── ChannelNameG.tiff\n",
    "    │   ├── ChannelNameH.tiff\n",
    "    │   ├── ChannelNameI.tiff\n",
    "    │   ├── ChannelNameJ.tiff\n",
    "    │   ├── ChannelNameK.tiff\n",
    "    │   ├── .\n",
    "    │   ├── .\n",
    "    │   └── .\n",
    "    └── slide_qwertyui\n",
    "        ├── ChannelNameA.tiff\n",
    "        ├── ChannelNameB.tiff\n",
    "        ├── ChannelNameC.tiff\n",
    "        ├── ChannelNameD.tiff\n",
    "        ├── ChannelNameE.tiff\n",
    "        ├── ChannelNameF.tiff\n",
    "        ├── ChannelNameG.tiff\n",
    "        ├── ChannelNameH.tiff\n",
    "        ├── ChannelNameI.tiff\n",
    "        ├── ChannelNameJ.tiff\n",
    "        ├── ChannelNameK.tiff\n",
    "        ├── .\n",
    "        ├── .\n",
    "        └── .\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might not want to perform the segmentation and analysis on all the slides/images. If that is the case you can adjust and filter for the subset of slides you want to analyse below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load_fovs"
    ]
   },
   "outputs": [],
   "source": [
    "# get all fovs in the folder...\n",
    "wsis = io_utils.list_folders(tiff_dir)\n",
    "\n",
    "# ... or optionally, select a specific set of fovs manually by name\n",
    "# wsis = [*np.asarray(wsis)[np.isin(wsis, [\"slide_name\"])]]\n",
    "print(wsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list the channel for all the slide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for wsi in tqdm(wsis):\n",
    "    print(wsi+\" channels:\")\n",
    "    print(io_utils.remove_file_extensions(io_utils.list_files(os.path.join(tiff_dir, wsi))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Perform hard thresholding for some of the channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! NOTE: You could perform hard thresholding at this point by modifying the single channel images before creating composite channels for segmentation in the next step. !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### you can manual implement hard thresholding of selected channel here and overwrite the respective single channel tiffs in the image_data subfolders with the thresholded image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load images and compute composite nuclear and membrane channels and save as `Mesmer` compatible input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the the *nuclear* `nuc` and *membrane* `mems` channels that will be used as input for the segmentation. Multiple *membrane* or *nuclear* channels will be collapsed into a singular channel, respectively. This can be useful if there is not a singular marker that is ubiquitously expressed by all cells in the tissue in question. At least one of `nucs` and `mems` can not be `None`. If the channel/channel names are exactly the same across all slides you can simple define an array of channel names as done by default below. Alternatively, if you want to use a different set of channels for each slide or the channel names are not consistent you will need to define a `dictionary` with and entry for each slide (e.g. `mems = {\"slide_asdfghjk\": ['HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIM'], \"slide_qwertyui\": ['HLAA', 'HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIMENTIN']}`). Note the difference naming of the `VIMENTIN` channel between the images and that `slide_asdfghjk` is missing the `HLAA` channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nuc_mem_set"
    ]
   },
   "outputs": [],
   "source": [
    "# nuclear channel name(s) (or nucs = None)\n",
    "nucs = ['DAPI_FINAL']\n",
    "# `nucs = {\"slide_asdfghjk\": ['DAPI_FINAL'],\n",
    "# \"slide_qwertyui\": ['DAPI_FINAL']}\n",
    "\n",
    "# # membrane channel name(s) (or mems = None)\n",
    "mems = ['HLAA', 'HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIM']\n",
    "# `mems = {\"slide_asdfghjk\": ['HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIM'],\n",
    "# \"slide_qwertyui\": ['HLAA', 'HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIMENTIN']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the `nuclear` and `membrane` markers, we create the composite nuclear-membrane `tiff` files which will used as the input to the `Deepcell` segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "gen_input"
    ]
   },
   "outputs": [],
   "source": [
    "# generate and save deepcell input tiffs\n",
    "# set img_sub_folder param to None if the image files in tiff_dir are not in a separate sub folder \n",
    "for wsi in tqdm(wsis):\n",
    "    nucs_channels = nucs if isinstance(nucs, dict) or nucs == None else nucs[wsi]\n",
    "    mems_channels = mems if isinstance(mems, dict) or mems == None else mems[wsi]\n",
    "\n",
    "    deepcell_service_utils.generate_deepcell_input(\n",
    "        deepcell_input_dir,\n",
    "        tiff_dir,\n",
    "        nucs_channels,\n",
    "        mems_channels,\n",
    "        [wsi],\n",
    "        img_sub_folder=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this step, we have creates a single `tiff` file per slide inside the `segmentation/deepcell_input` folder. This should looks similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "└── segmentation\n",
    "    └──  deepcell_input\n",
    "        ├── slide_asdfghjk.tiff\n",
    "        └── slide_qwertyui.tiff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Whole Slide Segmentation via `Deepcell`'s `Mesmer` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the the pre-trained `Mesmer` `MultiplexSegmentation` model provided by the `Deepcell` library. `Mesmer` was trained on 20X resolution data and thus depending on the our data we have to rescale the image by setting the `rescale_factor`, such that for 10X image `rescale_factor=2.0` or for a 60X image `rescale_factor=0.33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "seg_scale_set"
    ]
   },
   "outputs": [],
   "source": [
    "rescale_factor = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Load `Mesmer` segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pre-trained `Mesmer` model from the hardrive or redownload by setting `keras_model` to `keras_model = None`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! NOTE: Ignore NUMA node warnings. These warnings are docker/singularity container specific and can be safely ignored. !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set keras_model = None to redownload the latest MultiplexSegmentation\n",
    "keras_model = load_model(Path(\"/.keras/models/MultiplexSegmentation\"), compile=False)\n",
    "segmentation_model = Mesmer() if keras_model == None else Mesmer(model=keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final check and chance to adjust and filter the slides on which we will perform the cell segmentation on. By default it will only run the segmentation on the slides selected in **Section 2.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slide_file_names = np.asarray(io_utils.list_files(os.path.join(deepcell_input_dir)))\n",
    "slide_file_names = [*slide_file_names[np.isin(io_utils.remove_file_extensions(slide_file_names), wsis)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation function `predicted_tiled` takes various arguments that can/should be adapted for a given experiment:\n",
    "* overlap: the amount of pixels that tiles of the whole slide overlap during the segmentation in case the slide is too large to be processed and segmented all at once. Choose larger overlaps if the cells imaged are large to reduce artifacts.\n",
    "* cutoff: defines how close to a border of a segmentation tiles a cell has to be in order to be removed and inferred via `infer_gaps=True`\n",
    "* background_threshold: the maximum expected background signal for an area of the image/slide that is not containing any cells\n",
    "* infer_gaps: if set to `True` the boundary regions (see `cutoff`) of a segmentation tile are inferred from overlapping tiles (see `overlap`); set to `False` leaves boundaries between tiles unsegmented and the overlap setting will be ignored\n",
    "* compartment: the cellular compartment that will be segmented for; can be either \"whole-cell\", \"nuclear\" or \"both\"\n",
    "* cell_size_threshold: defines the minimum size of a segmented cells in pixels to be accepted as a \"cell\"; Default=None\n",
    "* app: `keras` model used for segmentation\n",
    "* pre_processing: pre-processing arguments passed on to the `deepcell` method (see https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#deepcell.applications.mesmer.Mesmer.predict)\n",
    "* post_processing: post-processing arguments passed on to `deepcell` method (see https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#deepcell.applications.mesmer.Mesmer.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment this notebook only officially supports `whole-cell` segmentation. If you run `nuclear` or `both` segmentation you may need to adjust some parts in the QC section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 200\n",
    "cutoff = 2\n",
    "background_threshold = 0.1\n",
    "infer_gaps = True\n",
    "compartment = \"whole-cell\"\n",
    "cell_size_threshold = None ## we perform cell size thresholding during the QC step below \n",
    "app = segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for slide_file_name in tqdm(slide_file_names):\n",
    "    slide = load_utils.load_imgs_from_dir(deepcell_input_dir, files = [slide_file_name], xr_channel_names=[\"nuclear\", \"membrane\"])\n",
    "    segmentation_mask = predict_tiled(slide.data, overlap=overlap, cutoff=cutoff, background_threshold=background_threshold, infer_gaps=infer_gaps, compartment=compartment, cell_size_threshold=cell_size_threshold, app=app)\n",
    "    save_model_output_wrapper(segmentation_mask, output_dir=deepcell_output_dir, feature_name=io_utils.remove_file_extensions([slide_file_name])[0],compartment=compartment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### 4.1 Visualise the segmented mask overlaid on the nuclear and membrane channels used for the segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the results of the segmentation, we can visualise the segmentation mask overlayed on the *nuclear* and *membrane* channel used for the segmentation. You can adjust the `id_to_visualise` variable below to plot result of a different slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create rgb overlay of image data for visualization\n",
    "id_to_visualise = 0\n",
    "segmentation_channels = load_utils.load_imgs_from_dir(deepcell_input_dir, files = [io_utils.list_files(os.path.join(deepcell_input_dir))[id_to_visualise]], xr_channel_names=[\"nuclear\", \"membran\"])\n",
    "segmentation_mask = load_utils.load_imgs_from_dir(deepcell_output_dir, files = [io_utils.list_files(os.path.join(deepcell_output_dir))[id_to_visualise]])\n",
    "rgb_images = create_rgb_image(np.asarray(segmentation_channels[:,:,:,:]), channel_colors=['green', 'blue'])\n",
    "overlay_segmentation = make_outline_overlay(rgb_data=rgb_images, predictions=segmentation_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fig, ax = plt.subplots(1 ,figsize=(15, 15))\n",
    "ax.imshow(overlay_segmentation[0, ...])\n",
    "ax.set_title('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save and visualise all segmentation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the segmentation mask overlay for each slide in the `segmentation/visualisation_no_qc` directory. You can also visualise a given channel on top of the segmentation by defining the `channel=` argument below with a list `[]` of channel names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save_mask"
    ]
   },
   "outputs": [],
   "source": [
    "# save the overlaid segmentation labels for each fov (these will not display, but will save in viz_dir)\n",
    "segmentation_utils.save_segmentation_labels(\n",
    "    segmentation_dir=deepcell_output_dir,\n",
    "    data_dir=deepcell_input_dir,\n",
    "    output_dir=deepcell_visualization_dir,\n",
    "    fovs=io_utils.remove_file_extensions(wsis),\n",
    "    channels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantification and Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Feature extraction and quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will perform feature extraction and quantification. For a full list of the features extracted, you can refer to the cell table section of at the `ark-analysis` [documentation](https://ark-analysis.readthedocs.io/en/latest/_rtd/data_types.html). Set the `nuclear_counts` and `fast_extraction` variables below to determine the level of information per cell that is being extracted and calculated. We generate both a size-normalised as well as a `arcsinh` transformed expression matrix for the segmented cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nuc_props_set"
    ]
   },
   "outputs": [],
   "source": [
    "# set to True to add nuclear cell properties to the expression matrix\n",
    "nuclear_counts = False\n",
    "\n",
    "# set to True to bypass expensive cell property calculations\n",
    "# only cell label, size, and centroid will be extracted if True\n",
    "fast_extraction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "create_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "# now extract the segmented imaging data to create normalized and transformed expression matrices\n",
    "# note that if you're loading your own dataset, please make sure all the imaging data is in the same folder\n",
    "# with each fov given its own folder and all fovs having the same channels\n",
    "cell_table_size_normalized = dict()\n",
    "cell_table_arcsinh_transformed = dict()\n",
    "\n",
    "for wsi in tqdm(wsis):\n",
    "    cell_table_size_normalized[wsi], cell_table_arcsinh_transformed[wsi] = \\\n",
    "        marker_quantification.generate_cell_table(segmentation_dir=deepcell_output_dir,\n",
    "                                                tiff_dir=tiff_dir,\n",
    "                                                img_sub_folder=None,\n",
    "                                                fovs=[wsi],\n",
    "                                                nuclear_counts=nuclear_counts,\n",
    "                                                fast_extraction=fast_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the size-normalised and `arcsinh` transformed expression matrices as `csv` files to hardrive. The compression of these `csv` files can optionally be set via `compression = `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the compression level if desired, ZSTD compression can offer up to a 60-70% reduction in file size.\n",
    "# NOTE: Compressed `csv` files cannot be opened in Excel. They must be uncompressed beforehand.\n",
    "compression = None\n",
    "\n",
    "# Uncomment the line below to allow for compressed `csv` files.\n",
    "# compression = {\"method\": \"zstd\", \"level\": 3}\n",
    "for wsi in tqdm(wsis):\n",
    "    cell_table_size_normalized[wsi].to_csv(os.path.join(cell_table_dir, 'cell_table_size_normalized_'+ wsi + '.csv'),\n",
    "                                    compression=compression, index=False)\n",
    "    cell_table_arcsinh_transformed[wsi].to_csv(os.path.join(cell_table_dir, 'cell_table_arcsinh_transformed_'+ wsi + '.csv'),\n",
    "                                        compression=compression, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Perform Cell DIVE specific QC (DAPI staining deviation and cell size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Additionally, we perform basic quality control to improve the quality of the data and account for Cell DIVE specific issues such as the loss of cells over multiple rounds of staining (large deviation of DAPI staining between first staining round and final staining round)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In particular we perform two QC steps:\n",
    "* remove any cell that is smaller than 50 pixel in area\n",
    "* remove any cell for which the first and last DAPI staining shows a deviation > 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_table_size_normalized_qc = dict()\n",
    "for wsi in tqdm(wsis):\n",
    "    cell_table_size_normalized_qc[wsi] = cell_table_size_normalized[wsi].loc[cell_table_size_normalized[wsi][\"cell_size\"] >= 50,]\n",
    "    cell_table_size_normalized_qc[wsi] = cell_table_size_normalized_qc[wsi].loc[abs(cell_table_size_normalized_qc[wsi][\"DAPI_INIT\"] - cell_table_size_normalized_qc[wsi][\"DAPI_FINAL\"])/cell_table_size_normalized_qc[wsi][\"DAPI_INIT\"] <= 0.5, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_table_arcsinh_transformed_qc = dict()\n",
    "\n",
    "for wsi in tqdm(wsis):\n",
    "    array_mask = np.isin(np.array(cell_table_arcsinh_transformed[wsi][\"label\"]), np.array(cell_table_size_normalized_qc[wsi][\"label\"]))\n",
    "    df = pd.DataFrame.copy(cell_table_arcsinh_transformed[wsi])\n",
    "    df = pd.DataFrame(df.loc[array_mask,])\n",
    "    cell_table_arcsinh_transformed_qc[wsi] = df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a quick sanity check that both the normalised and transformed data after QC still contain the same cell IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for wsi in wsis:\n",
    "    print(wsi + \":\")\n",
    "    print(np.array(cell_table_size_normalized_qc[wsi][\"label\"]).sort() == np.array(cell_table_arcsinh_transformed_qc[wsi][\"label\"]).sort())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the cleaned expression matrices as `csv` files to hardrive. The compression of these files can optionally be set via `compression=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression = None\n",
    "\n",
    "for wsi in tqdm(wsis):\n",
    "    cell_table_size_normalized_qc[wsi].to_csv(os.path.join(cell_table_dir_qc, 'cell_table_size_normalized_'+ wsi + '.csv'),\n",
    "                                    compression=compression, index=False)\n",
    "    cell_table_arcsinh_transformed_qc[wsi].to_csv(os.path.join(cell_table_dir_qc, 'cell_table_arcsinh_transformed_'+ wsi + '.csv'),\n",
    "                                        compression=compression, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we also apply QC results to the segmentation masks. We remove the filtered cells identified in the above steps from the segmentation mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wsi in tqdm(wsis):\n",
    "    \n",
    "    tmp_ids_qc = np.array(cell_table_size_normalized_qc[wsi][\"label\"])\n",
    "\n",
    "    tmp_segmentation_mask = load_utils.load_imgs_from_dir(deepcell_output_dir, files = [wsi + \"_whole_cell.tiff\"])\n",
    "\n",
    "    tmp_segmentation_mask = np.where(np.isin(tmp_segmentation_mask, tmp_ids_qc, invert=True), 0, tmp_segmentation_mask.data)\n",
    "\n",
    "    save_model_output_wrapper(tmp_segmentation_mask, output_dir=deepcell_output_dir_qc, feature_name=wsi,compartment=compartment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we save the updated segmentation mask overlay after QC for each slide in the `segmentation/visualisation` directory. You can also visualise a given channel on top of the segmentation by defining the `channel=` argument below with a list `[]` of channel names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the overlaid segmentation labels for each fov after QC (these will not display, but will save in viz_dir)\n",
    "segmentation_utils.save_segmentation_labels(\n",
    "    segmentation_dir=deepcell_output_dir_qc,\n",
    "    data_dir=deepcell_input_dir,\n",
    "    output_dir=deepcell_visualization_dir_qc,\n",
    "    fovs=io_utils.remove_file_extensions(wsis),\n",
    "    channels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have something that resembles the following folder structure and files. This folder/data structure should be readibly usable with the `ark-analysis` [pipeline](https://ark-analysis.readthedocs.io/en/latest/) notebooks starting from [Pixel clustering with Pixie](https://github.com/angelolab/ark-analysis#2-pixel-clustering-with-pixie) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```markdown\n",
    "└── segmentation\n",
    "    ├── cell_table\n",
    "    │   ├── cell_table_arcsinh_transformed.csv\n",
    "    │   └── cell_table_size_normalized.csv\n",
    "    ├── cell_table_no_qc\n",
    "    │   ├── cell_table_arcsinh_transformed.csv\n",
    "    │   └── cell_table_size_normalized.csv\n",
    "    ├── deepcell_input\n",
    "    │   ├── slide_asdfghjk.tiff\n",
    "    │   └── slide_qwertyui.tiff\n",
    "    ├── deepcell_output\n",
    "    │   ├── slide_asdfghjk_whole_cell.tiff\n",
    "    │   └── slide_qwertyui_whole_cell.tiff\n",
    "    ├── deepcell_output_no_qc\n",
    "    │   ├── slide_asdfghjk_whole_cell.tiff\n",
    "    │   └── slide_qwertyui_whole_cell.tiff\n",
    "    ├── deepcell_visualization\n",
    "    │   ├── slide_asdfghjk_segmentation_borders.tiff\n",
    "    │   └── slide_qwertyui_segmentation_borders.tiff\n",
    "    └── deepcell_visualization_no_qc\n",
    "        ├── slide_asdfghjk_segmentation_borders.tiff\n",
    "        └── slide_qwertyui_segmentation_borders.tiff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Create output that can be used in other pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will provide code to transform/translate the above segmentation result into different formats and structure so they can be used with other existing analysis pipeline. You can also use it to define your own transformation/restructuring function. By default an output compatible with the `ark analysis` toolkit is being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ark_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spooxs_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_output = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Ark analysis toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ark analysis pipeline assumes all extracted features to be stored in single dataframe and thus we have to harmonize the channel names if necessary and remove channels that do not appear in all slides. The default below assumes that all images have exactly the same channels and used the same naming scheme. You can modify the marker names by defining key-value pairs (e.g. `\"old_channel_name\":\"new_channel_name\"`) in the `channel_rename_dictionary` below. Not dealing with mismatched marker names or removing markers that are not present in all images may lead to issue further downstream in the analysis. If you want proceed with mismatched column names regardless you can set `ignore_na_warning=True`. By default, we will remove markers that do not appear in all slides, if you would like to keep all markers in the final quantification table, acknowledging that this will introduce `NA`s, set `remove_non_shared_markers=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modify or adjust channel names if necessary\n",
    "marker_rename_dictionary = {    \n",
    "# \"old_marker_name\":\"new_marker_name\"\n",
    "}\n",
    "\n",
    "for wsi in wsis:\n",
    "    cell_table_size_normalized[wsi].rename(columns=marker_rename_dictionary, inplace=True)\n",
    "    cell_table_arcsinh_transformed[wsi].rename(columns=marker_rename_dictionary, inplace=True)\n",
    "    cell_table_size_normalized_qc[wsi].rename(columns=marker_rename_dictionary, inplace=True)\n",
    "    cell_table_arcsinh_transformed_qc[wsi].rename(columns=marker_rename_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_na_warning = False\n",
    "remove_non_shared_markers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ark_output:\n",
    "    ## non qc\n",
    "    combined_cell_table_size_normalized = pd.concat(cell_table_size_normalized)\n",
    "    combined_cell_table_arcsinh_transformed = pd.concat(cell_table_arcsinh_transformed)\n",
    "\n",
    "    ## qc expression table\n",
    "    combined_cell_table_size_normalized_qc = pd.concat(cell_table_size_normalized_qc)\n",
    "    combined_cell_table_arcsinh_transformed_qc = pd.concat(cell_table_arcsinh_transformed_qc)\n",
    "    \n",
    "    if (combined_cell_table_size_normalized.isnull().values.any() or combined_cell_table_arcsinh_transformed.isnull().values.any()):\n",
    "        if ignore_na_warning == False:\n",
    "            warnings.warn(\"Combining expression table introduced NA indicating that some marker are not present in all images or that markers are not consistently named across images. To proceed anyway set `ignore_na_warning=True` and rerun this step. Please, note that this most likely will lead to issue downstream. The better solution is to harmonise the marker names (using the previous step) to resolve this issuee.\");\n",
    "            continue_to_save = False\n",
    "        elif ignore_na_warning == True and remove_non_shared_markers == False:\n",
    "            warnings.warn(\"Combining expression table introduced NA and you have set `ignore_na_warning=True` to proceed anyway. Please, note that this most likely will lead to issue downstream.\");  \n",
    "            continue_to_save = True\n",
    "        elif ignore_na_warning == True and remove_non_shared_markers == True:\n",
    "            warnings.warn(\"Combining expression table introduced NA and you have set `remove_non_shared_markers=True` to proceed while removing the markers not present in all images.\");  \n",
    "            combined_cell_table_size_normalized.dropna(axis='columns', inplace=True)\n",
    "            combined_cell_table_arcsinh_transformed.dropna(axis='columns', inplace=True)\n",
    "            combined_cell_table_size_normalized_qc.dropna(axis='columns', inplace=True)\n",
    "            combined_cell_table_arcsinh_transformed_qc.dropna(axis='columns', inplace=True)\n",
    "            continue_to_save = True\n",
    "    else:\n",
    "        continue_to_save = True\n",
    "        \n",
    "        \n",
    "    if continue_to_save:\n",
    "        ## save combined feature matrices to the hard drive\n",
    "        combined_cell_table_size_normalized_qc.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_size_normalized.csv'),\n",
    "                                        compression=compression, index=False)\n",
    "        combined_cell_table_arcsinh_transformed_qc.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_arcsinh_transformed.csv'),\n",
    "                                            compression=compression, index=False)\n",
    "\n",
    "        combined_cell_table_size_normalized.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_size_normalized.csv'),\n",
    "                                        compression=compression, index=False)\n",
    "        combined_cell_table_arcsinh_transformed_qc.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_arcsinh_transformed.csv'),\n",
    "                                            compression=compression, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Spooxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if spooxs_output:\n",
    "    print(\"Work in progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if other_output:\n",
    "    ## Implement your own conversion code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd428f2623867f362c6ffd1805d28fe273bb79d15f4a3a73107e7f51d98be79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
