{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Whole slide segmentation of Cell DIVE multiplex images using `DeepCell`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: red\"> !!NOTE!! THIS NOTEBOOK ACTS AS A TEMPLATE AND WILL NOT SAFE CHANGES UNLESS YOU SAVE A COPY TO YOUR OWN FOLDER. USE `FILE->DOWNLOAD` IN THE TASKBAR AT THE VERY TOP OF THIS PAGE TO SAVE A COPY OF THE NOTEBOOK. THEN, USING THE SIDEBAR TO THE RIGHT OPEN THE NEW COPY OF THE NOTEBOOK BEFORE YOU GET STARTED (RUNNING ON WSL C: AND D: DRIVES SHOULD BE LOCATED UNDER `drives/c` AND `drives/d`, RESPECTIVELY) !!NOTE!!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run through this notebook **step-by-step** and adjust the code if necessary at any point in the process. The explaination and comments througout the notebook will help and guide through the proces. There will also be pointers as to what modification to a particular you might want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this notebook is adapted from and consequently loosely follows the structure and logic of the segmentation notebook of the `ark-analysis` toolset. Thus, it makes use some of the `ark-analysis` tooling as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We load all the necessary libraries used for the segmentation, visualisation and quantification in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as io\n",
    "import xarray as xr\n",
    "from alpineer import io_utils, load_utils, image_utils\n",
    "\n",
    "from ark.segmentation import marker_quantification, segmentation_utils\n",
    "from ark.utils import (deepcell_service_utils, plot_utils)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from deepcell.applications import Mesmer\n",
    "from deepcell.utils.plot_utils import make_outline_overlay\n",
    "from deepcell.utils.plot_utils import create_rgb_image\n",
    "\n",
    "from wsi_segmentation.segmentation_utils import predict_tiled\n",
    "from wsi_segmentation.io_utils import save_model_output_wrapper\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Define data location and setup directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Set data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to set the *data/base directory* `base_dir` that contains a folder called `ome_tiff` with 1 *ome.tiff* per slide. The content of the *base* directory would look something like example below. We will also save the results and all the pipeline outputs in this folder as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "└── ome_tiff\n",
    "    ├── slide_asdfghjk.ome.tiff\n",
    "    └── slide_qwertyui.ome.tiff\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define the *data/base directory* we have two options:\n",
    "* set `use_filechooser=True` and use the widget to find and identify the data/base directory\n",
    "* set `use_filechooser=False` and manual define the data/base directory below via `data_dir=`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: In `WSL` the `C:` drive, `D:` drive, etc are mounted and located at `/mnt/c`, `/mnt/d`, etc, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_filechooser = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if use_filechooser:\n",
    "    fc = FileChooser(\"\")\n",
    "    fc.show_only_dirs = True\n",
    "    fc.title = '<b>Choose data folder that contains `ome_tiff` folder with 1 ome_tiff per slide</b>'\n",
    "    display(fc)\n",
    "else:\n",
    "    data_dir = \"/path/to/data/directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "base_dir"
    ]
   },
   "outputs": [],
   "source": [
    "# set the base directory\n",
    "base_dir = fc.selected_path if use_filechooser == True else data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup intermediate and results paths\n",
    "\n",
    "Next, we create all the required output and results directories such that we have the following folder structure in the `base directory`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── ome_tiff\n",
    "├── image_data\n",
    "└── segmentation\n",
    "    ├── cell_table\n",
    "    ├── cell_table_raw\n",
    "    ├── deepcell_input\n",
    "    ├── deepcell_output\n",
    "    └── deepcell_visualization\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set all the directory names to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "file_path"
    ]
   },
   "outputs": [],
   "source": [
    "ome_tiff_dir = os.path.join(base_dir, \"ome_tiff\")\n",
    "tiff_dir = os.path.join(base_dir, \"image_data\")\n",
    "cell_table_dir = os.path.join(base_dir, \"segmentation/cell_table_raw\")\n",
    "cell_table_dir_qc = os.path.join(base_dir, \"segmentation/cell_table\")\n",
    "deepcell_input_dir = os.path.join(base_dir, \"segmentation/deepcell_input\")\n",
    "deepcell_output_dir = os.path.join(base_dir, \"segmentation/deepcell_output\")\n",
    "deepcell_visualization_dir = os.path.join(base_dir, \"segmentation/deepcell_visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check if the directoies exist and if not create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create_dirs"
    ]
   },
   "outputs": [],
   "source": [
    "for directory in [tiff_dir, cell_table_dir, cell_table_dir_qc, deepcell_input_dir, deepcell_output_dir, deepcell_visualization_dir ]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, validate that all the directories have been created correctly, so we can proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "validate_path"
    ]
   },
   "outputs": [],
   "source": [
    "# validate paths\n",
    "io_utils.validate_paths([base_dir,\n",
    "                         tiff_dir,\n",
    "                         deepcell_input_dir,\n",
    "                         deepcell_output_dir,\n",
    "                         cell_table_dir,\n",
    "                         cell_table_dir_qc,\n",
    "                         deepcell_visualization_dir\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Split OME TIFF files into single-channel TIFFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have obtained multichannel `ome.tiff` files from the Cell DIVE platform, we have to split them into single channel `tiff` files. To achieve this, set `split_ome = True` and split the `ome.tiff` by running the below steps. If you have run this step previously and already split up the `ome.tiff` files you can set `split_ome = False` to skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alternatively, you can also manually create one folder per image/slide containing single channel `tiff`s inside the `image_data` folder in the `base/data folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_ome = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if split_ome:\n",
    "    # Gather all the OME-TIFFs\n",
    "    ome_dir = pathlib.Path(ome_tiff_dir)\n",
    "    ome_tiffs = list(ome_dir.glob(\"*.ome.[tif tiff]*\"))\n",
    "\n",
    "    os.listdir(ome_tiff_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Convert all the OME-TIFFs to Single Channel TIFFs\n",
    "if split_ome:\n",
    "    for ome_tiff in tqdm(ome_tiffs):\n",
    "        load_utils.ome_to_fov(ome_tiff, data_dir=tiff_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these steps we should have one directory per slide in the `image_data` directory containing single `.tiff` files per channel. Looking similiar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "├── ome_tiff\n",
    "│   ├── slide_asdfghjk.ome.tiff\n",
    "│   └── slide_qwertyui.ome.tiff\n",
    "└── image_data\n",
    "    ├── slide_asdfghjk\n",
    "    │   ├── ChannelNameA.tiff\n",
    "    │   ├── ChannelNameB.tiff\n",
    "    │   ├── ChannelNameC.tiff\n",
    "    │   ├── ChannelNameD.tiff\n",
    "    │   ├── ChannelNameE.tiff\n",
    "    │   ├── ChannelNameF.tiff\n",
    "    │   ├── ChannelNameG.tiff\n",
    "    │   ├── ChannelNameH.tiff\n",
    "    │   ├── ChannelNameI.tiff\n",
    "    │   ├── ChannelNameJ.tiff\n",
    "    │   ├── ChannelNameK.tiff\n",
    "    │   ├── .\n",
    "    │   ├── .\n",
    "    │   └── .\n",
    "    └── slide_qwertyui\n",
    "        ├── ChannelNameA.tiff\n",
    "        ├── ChannelNameB.tiff\n",
    "        ├── ChannelNameC.tiff\n",
    "        ├── ChannelNameD.tiff\n",
    "        ├── ChannelNameE.tiff\n",
    "        ├── ChannelNameF.tiff\n",
    "        ├── ChannelNameG.tiff\n",
    "        ├── ChannelNameH.tiff\n",
    "        ├── ChannelNameI.tiff\n",
    "        ├── ChannelNameJ.tiff\n",
    "        ├── ChannelNameK.tiff\n",
    "        ├── .\n",
    "        ├── .\n",
    "        └── .\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filter slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might not want to perform the segmentation and analysis on all the slides/images. If that is the case you can adjust and filter for the subset of slides you want to analyse below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load_fovs"
    ]
   },
   "outputs": [],
   "source": [
    "# get all fovs in the folder...\n",
    "wsis = io_utils.list_folders(tiff_dir)\n",
    "\n",
    "# ... or optionally, select a specific set of fovs manually by name\n",
    "# wsis = [*np.asarray(wsis)[np.isin(wsis, [\"slide_name\"])]]\n",
    "print(wsis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list all the channel of the first slide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "io_utils.remove_file_extensions(io_utils.list_files(os.path.join(tiff_dir, wsis[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Perform hard thresholding for some of the channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! NOTE: You could perform hard thresholding at this point by modifying the single channel images before creating composite channels for segmentation in the next step. !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### you can manual implement hard thresholding of selected channel here and overwrite the respective single channel tiffs in the image_data subfolders with the thresholded image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load images and compute composite nuclear and membrane channels and save as `Mesmer` compatable input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the the *nuclear* `nuc` and *membrane* `mems` channels that will be used as input for the segmentation. Multiple *membrane* or *nuclear* channels will be collapsed into a singular channel, respectively. This can be useful if there is not a singular marker that is ubiquitously expressed by all cells in the tissue in question. At least one of `nucs` and `mems` can not be `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nuc_mem_set"
    ]
   },
   "outputs": [],
   "source": [
    "# nuclear channel name(s) (or nucs = None)\n",
    "# nucs = ['DAPI_FINAL']\n",
    "nucs = ['H3K27me3', 'H3K9ac']\n",
    "\n",
    "# # membrane channel name(s) (or mems = None)\n",
    "# mems = ['HLAA', 'HLADR', 'CD68', 'CK8', 'CD3', 'CD45', 'VIM']\n",
    "mems = ['CD14', 'CD45', 'ECAD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the `nuclear` and `membrane` markers, we create the composite nuclear-membrane `tiff` files which will used as the input to the `Deepcell` segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "gen_input"
    ]
   },
   "outputs": [],
   "source": [
    "# generate and save deepcell input tiffs\n",
    "# set img_sub_folder param to None if the image files in tiff_dir are not in a separate sub folder \n",
    "deepcell_service_utils.generate_deepcell_input(\n",
    "    deepcell_input_dir,\n",
    "    tiff_dir,\n",
    "    nucs,\n",
    "    mems,\n",
    "    wsis,\n",
    "    img_sub_folder=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this step, we have creates a single `tiff` file per slide inside the `segmentation/deepcell_input` folder. This should looks similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "└── segmentation\n",
    "    └──  deepcell_input\n",
    "        ├── slide_asdfghjk.tiff\n",
    "        └── slide_qwertyui.tiff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Whole Slide Segmentation via `Deepcell`'s `Mesmer` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the the pre-trained `Mesmer` `MultiplexSegmentation` model provided by the `Deepcell` library. `Mesmer` was trained on 20X resolution data and thus depending on the our data we have to rescale the image by setting the `rescale_factor`, such that for 10X image `rescale_factor=2.0` or for a 60X image `rescale_factor=0.33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "seg_scale_set"
    ]
   },
   "outputs": [],
   "source": [
    "rescale_factor = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Load `Mesmer` segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pre-trained `Mesmer` model from the hardrive or redownload by setting `keras_model` to `keras_model = None`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! NOTE: Ignore NUMA node warnings. These warnings are docker/singularity container specific and can be safely ignored. !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set keras_model = None to redownload the latest MultiplexSegmentation\n",
    "keras_model = load_model(Path(\"/.keras/models/MultiplexSegmentation\") )\n",
    "segmentation_model = Mesmer() if keras_model == None else Mesmer(model=keras_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Run segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final check and chance to adjust and filter the slides on which we will perform the cell segmentation on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slide_file_names = io_utils.list_files(os.path.join(deepcell_input_dir))\n",
    "slide_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation function `predicted_tiled` takes various arguments that can/should be adapted for a given experiment:\n",
    "* overlap: the amount of pixels that tiles of the whole slide overlap during the segmentation in case the slide is too large to be processed and segmented all at once. Choose larger overlaps if the cells imaged are large to reduce artifacts.\n",
    "* cutoff: defines how close to a border of a segmentation tiles a cell has to be in order to be removed and inferred via `infer_gaps=True`\n",
    "* background_threshold: the maximum expected background signal for an area of the image/slide that is not containing any cells\n",
    "* infer_gaps: if set to `True` the boundary regions (see `cutoff`) of a segmentation tile are inferred from overlapping tiles (see `overlap`); set to `False` leaves boundaries between tiles unsegmented and the overlap setting will be ignored\n",
    "* compartment: the cellular compartment that will be segmented for; can be either \"whole-cell\", \"nuclear\" or \"both\"\n",
    "* cell_size_threshold: defines the minimum size of a segmented cells in pixels to be accepted as a \"cell\"; Default=None\n",
    "* app: `keras` model used for segmentation\n",
    "* pre_processing: pre-processing arguments passed on to the `deepcell` method (see https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#deepcell.applications.mesmer.Mesmer.predict)\n",
    "* post_processing: post-processing arguments passed on to `deepcell` method (see https://deepcell.readthedocs.io/en/master/API/deepcell.applications.html#deepcell.applications.mesmer.Mesmer.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment this notebook only officially supports `whole-cell` segmentation. If you run `nuclear` or `both` segmentation you may need to adjust some parts in the QC section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 200\n",
    "cutoff = 2\n",
    "background_threshold = 0.1\n",
    "infer_gaps = True\n",
    "compartment = \"whole-cell\"\n",
    "cell_size_threshold = 50\n",
    "app = segmentation_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for slide_file_name in tqdm(slide_file_names):\n",
    "    slide = load_utils.load_imgs_from_dir(deepcell_input_dir, files = [slslide_file_nameide_name], xr_channel_names=[\"nuclear\", \"membran\"])\n",
    "    segmentation_mask = predict_tiled(slide, overlap=overlap, cutoff=cutoff, background_threshold=background_threshold, infer_gaps=infer_gaps, compartment=compartment, cell_size_threshold=cell_size_threshold, app=app)\n",
    "    save_model_output_wrapper(segmentation_mask, output_dir=deepcell_output_dir, feature_name=io_utils.remove_file_extensions([slide_file_name])[0],compartment=compartment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### 4.1 Visualise the segmented mask overlaid on the nuclear and membrane channels used for the segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the results of the segmentation, we can visualise the segmentation mask overlayed on the *nuclear* and *membrane* channel used for the segmentation. You can adjust the `id_to_visualise` variable below to plot result of a different slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create rgb overlay of image data for visualization\n",
    "id_to_visualise = 0\n",
    "segmentation_channels = load_utils.load_imgs_from_dir(deepcell_input_dir, files = [io_utils.list_files(os.path.join(deepcell_input_dir))[id_to_visualise]], xr_channel_names=[\"nuclear\", \"membran\"])\n",
    "segmentation_mask = load_utils.load_imgs_from_dir(deepcell_output_dir, files = [io_utils.list_files(os.path.join(deepcell_output_dir))[id_to_visualise]])\n",
    "rgb_images = create_rgb_image(np.asarray(segmentation_channels[:,:,:,:]), channel_colors=['green', 'blue'])\n",
    "overlay_segmentation = make_outline_overlay(rgb_data=rgb_images, predictions=segmentation_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select index for displaying\n",
    "idx = 0\n",
    "\n",
    "# plot the data\n",
    "fig, ax = plt.subplots(1 ,figsize=(15, 15))\n",
    "ax.imshow(overlay_segmentation[idx, ...])\n",
    "ax.set_title('Predictions tiled overlap version (boundaries inferred)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save and visualise all segmentation outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save all the segmentation mask overlay for each slide in the `segmentation/visualisation` directory. You can also visualise a given channel on top of the segmentation by defining the `channel=` argument below with a list `[]` of channel names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save_mask"
    ]
   },
   "outputs": [],
   "source": [
    "# save the overlaid segmentation labels for each fov (these will not display, but will save in viz_dir)\n",
    "segmentation_utils.save_segmentation_labels(\n",
    "    segmentation_dir=deepcell_output_dir,\n",
    "    data_dir=deepcell_input_dir,\n",
    "    output_dir=deepcell_visualization_dir,\n",
    "    fovs=io_utils.remove_file_extensions(wsis),\n",
    "    channels=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantification and Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.1 Feature extraction and quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we will perform feature extraction and quantification. For a full list of the features extracted, you can refer to the cell table section of at the `ark-analysis` [documentation](https://ark-analysis.readthedocs.io/en/latest/_rtd/data_types.html). Set the `nuclear_counts` and `fast_extraction` variables below to determine the level of information per cell that is being extracted and calculated. We generate both a size-normalised as well as a `arcsinh` transformed expression matrix for the segmented cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nuc_props_set"
    ]
   },
   "outputs": [],
   "source": [
    "# set to True to add nuclear cell properties to the expression matrix\n",
    "nuclear_counts = False\n",
    "\n",
    "# set to True to bypass expensive cell property calculations\n",
    "# only cell label, size, and centroid will be extracted if True\n",
    "fast_extraction = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "create_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "# now extract the segmented imaging data to create normalized and transformed expression matrices\n",
    "# note that if you're loading your own dataset, please make sure all the imaging data is in the same folder\n",
    "# with each fov given its own folder and all fovs having the same channels\n",
    "cell_table_size_normalized, cell_table_arcsinh_transformed = \\\n",
    "    marker_quantification.generate_cell_table(segmentation_dir=deepcell_output_dir,\n",
    "                                              tiff_dir=tiff_dir,\n",
    "                                              img_sub_folder=None,\n",
    "                                              fovs=wsis,\n",
    "                                              batch_size=5,\n",
    "                                              nuclear_counts=nuclear_counts,\n",
    "                                              fast_extraction=fast_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save the size-normalised and `arcsinh` transformed expression matrices as `csv` files to hardrive. The compression of these `csv` files can optionally be set via `compression = `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save_exp_mat"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the compression level if desired, ZSTD compression can offer up to a 60-70% reduction in file size.\n",
    "# NOTE: Compressed `csv` files cannot be opened in Excel. They must be uncompressed beforehand.\n",
    "compression = None\n",
    "\n",
    "# Uncomment the line below to allow for compressed `csv` files.\n",
    "# compression = {\"method\": \"zstd\", \"level\": 3}\n",
    "\n",
    "cell_table_size_normalized.to_csv(os.path.join(cell_table_dir, 'cell_table_size_normalized.csv'),\n",
    "                                  compression=compression, index=False)\n",
    "cell_table_arcsinh_transformed.to_csv(os.path.join(cell_table_dir, 'cell_table_arcsinh_transformed.csv'),\n",
    "                                      compression=compression, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Perform Cell DIVE specific QC (DAPI staining deviation and cell size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Additionally, we perform basic quality control to improve the quality of the data and account for Cell DIVE specific issues such as the loss of cells over multiple rounds of staining (large deviation of DAPI staining between first staining round and final staining round)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In particular we perform two QC steps:\n",
    "* remove any cell that is smaller than 50 pixel in area\n",
    "* remove any cell for which the first and last DAPI staining shows a deviation > 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cell_table_size_normalized_qc = cell_table_size_normalized.loc[cell_table_size_normalized[\"cell_size\"] >= 50,]\n",
    "cell_table_size_normalized_qc = cell_table_size_normalized_qc.loc[abs(cell_table_size_normalized_qc[\"DAPI_INIT\"] - cell_table_size_normalized_qc[\"DAPI_FINAL\"])/cell_table_size_normalized_qc[\"DAPI_INIT\"] <= 0.5, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_arcsinh_transformed_qc = []\n",
    "\n",
    "for j in wsis:\n",
    "    array_mask = np.isin(np.array(cell_table_arcsinh_transformed.loc[cell_table_arcsinh_transformed[\"fov\"] == j, \"label\"]),  np.array(cell_table_size_normalized_qc.loc[cell_table_size_normalized_qc[\"fov\"] == j, \"label\"]))\n",
    "    \n",
    "    df = pd.DataFrame.copy(cell_table_arcsinh_transformed)\n",
    "    df = pd.DataFrame(df.loc[df[\"fov\"] == j,].loc[array_mask,])\n",
    "    list_arcsinh_transformed_qc.append(df,) \n",
    "\n",
    "cell_table_arcsinh_transformed_qc = pd.concat(list_arcsinh_transformed_qc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a quick sanity check that both the normalised and transformed data after QC still contain the same cell IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(cell_table_arcsinh_transformed_qc[\"fov\"] + [str(j) for j in cell_table_arcsinh_transformed_qc[\"label\"]]).sort() == np.array(cell_table_size_normalized_qc[\"fov\"] + [str(j) for j in cell_table_size_normalized_qc[\"label\"]]).sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the cleaned expression matrices as `csv` files to hardrive. The compression of these files can optionally be set via `compression=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression = None\n",
    "cell_table_size_normalized_qc.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_size_normalized.csv'),\n",
    "                                  compression=compression, index=False)\n",
    "cell_table_arcsinh_transformed_qc.to_csv(os.path.join(cell_table_dir_qc, 'cell_table_arcsinh_transformed.csv'),\n",
    "                                      compression=compression, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we also apply QC results to the segmentation masks. After copying the original masks into `deepcell_output_raw`, we remove the filtered cells identified in the above steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copytree(deepcell_output_dir, deepcell_output_dir + \"_raw\", dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask_file_names = io_utils.list_files(os.path.join(deepcell_output_dir))\n",
    "\n",
    "for slide_name in tqdm(io_utils.remove_file_extensions(slide_file_names)):\n",
    "    \n",
    "    tmp_ids_qc = np.array(cell_table_size_normalized_qc.loc[cell_table_size_normalized_qc[\"fov\"] == slide_name, \"label\"])\n",
    "\n",
    "    tmp_segmentation_mask = load_utils.load_imgs_from_dir(deepcell_output_dir + \"_raw\", files = [slide_name + \"_whole_cell.tiff\"])\n",
    "\n",
    "    tmp_segmentation_mask = np.where(np.isin(tmp_segmentation_mask, tmp_ids_qc, invert=True), 0, tmp_segmentation_mask.data)\n",
    "\n",
    "    save_model_output_wrapper(tmp_segmentation_mask, output_dir=deepcell_output_dir, feature_name=io_utils.remove_file_extensions([slide_name])[0],compartment=compartment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have something that resembles the following folder structure and files. This folder/data structure should be readibly usable with the `ark-analysis` [pipeline](https://ark-analysis.readthedocs.io/en/latest/) notebooks starting from [Pixel clustering with Pixie](https://github.com/angelolab/ark-analysis#2-pixel-clustering-with-pixie) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```markdown\n",
    "└── segmentation\n",
    "    ├── cell_table\n",
    "    │   ├── cell_table_arcsinh_transformed.csv\n",
    "    │   └── cell_table_size_normalized.csv\n",
    "    ├── cell_table_raw\n",
    "    │   ├── cell_table_arcsinh_transformed.csv\n",
    "    │   └── cell_table_size_normalized.csv\n",
    "    ├── deepcell_input\n",
    "    │   ├── slide_asdfghjk.tiff\n",
    "    │   └── slide_qwertyui.tiff\n",
    "    ├── deepcell_output\n",
    "    │   ├── slide_asdfghjk_whole_cell.tiff\n",
    "    │   └── slide_qwertyui_whole_cell.tiff\n",
    "    └── deepcell_visualization\n",
    "        ├── slide_asdfghjk_segmentation_borders.tiff\n",
    "        └── slide_qwertyui_segmentation_borders.tiff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Create output that can be used in other pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will provide code to transform/translate the above segmentation result into different formats and structure so they can be used with other existing analysis pipeline. You can also use it to define your own transformation/restructuring function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spooxs_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "other_output = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Spooxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if spooxs_output:\n",
    "    print(\"To be implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if other_output:\n",
    "    print(\"To be implemented.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd428f2623867f362c6ffd1805d28fe273bb79d15f4a3a73107e7f51d98be79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
